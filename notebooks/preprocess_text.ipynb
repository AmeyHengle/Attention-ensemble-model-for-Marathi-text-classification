{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    \n",
    "        # --------------------------------------- Constructor --------------------------------------- \n",
    "        \n",
    "        def __init__(self,stopword_list):\n",
    "            self.data_path = ''\n",
    "            self.stopword_list = stopword_list\n",
    "        \n",
    "        \n",
    "        # --------------------------------------- Preprocess --------------------------------------- \n",
    "        \n",
    "        def clean_text(self,text):\n",
    "            \n",
    "            special_chars = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "            text = str(text)\n",
    "            \n",
    "            # Lemmatizing English words\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            stemmer = PorterStemmer() \n",
    "            \n",
    "            # Cleaning the urls\n",
    "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "            # Cleaning the html elements\n",
    "            text = re.sub(r'<.*?>', '', text)\n",
    "            \n",
    "            # Removing the punctuations\n",
    "            text = re.sub('[!#?,.:\";-@#$%^&*_~<>()-]', ' ', text)\n",
    "                    \n",
    "            # Removing stop words\n",
    "            text = ' '.join([word for word in text.split() if word not in self.stopword_list])\n",
    "            \n",
    "            preprocessed_text = \"\"\n",
    "            \n",
    "            for word in text.split(): \n",
    "#                 if any(chr.isalpha() for chr in word) and any(chr.isdigit() for chr in word): \n",
    "                    if (re.match('^[0-9]+$', word)):\n",
    "                       if(word.isnumeric()):\n",
    "                           preprocessed_text = preprocessed_text + '<Numeric>' + \" \"\n",
    "                    \n",
    "                    else:\n",
    "                            if re.match('[a-zA-Z]+', word):\n",
    "                                word = word.lower()\n",
    "                                word = lemmatizer.lemmatize(word)\n",
    "                                preprocessed_text = preprocessed_text + word + \" \"\n",
    "                                \n",
    "                            else:\n",
    "                                preprocessed_text = preprocessed_text + word + \" \"\n",
    "                       \n",
    "\n",
    "            # Lemmatizing Marathi words (optional)\n",
    "#             for word in text:\n",
    "#                 if not re.match('[a-zA-Z]+',word) and word != '#s':\n",
    "                    \n",
    "            \n",
    "            \n",
    "            return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
