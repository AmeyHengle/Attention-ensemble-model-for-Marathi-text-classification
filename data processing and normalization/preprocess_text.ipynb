{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "        \n",
    "        # --------------------------------------- Constructor --------------------------------------- \n",
    "        \n",
    "        def __init__(self,stopword_list):\n",
    "            self.data_path = ''\n",
    "            self.stopword_list = stopword_list\n",
    "                \n",
    "\n",
    "        # --------------------------------------- Preprocess --------------------------------------- \n",
    "        \n",
    "        def expand_concatenations(self, word):\n",
    "            \n",
    "            \n",
    "            if not re.match('[a-zA-Z]+', word) or re.match('/d+',word):\n",
    "                for i in range(len(word)):\n",
    "                    if not('DEVANAGARI ' in unicodedata.name(word[i])):\n",
    "                        word = word[:i] if( len(word[i:]) < 2 and not word[i:].isnumeric()) else word[:i] + \" \" + word[i:]\n",
    "                        break\n",
    "            else:\n",
    "                for i in range(len(word)):\n",
    "                    if ('DEVANAGARI ' in unicodedata.name(word[i])):\n",
    "                        word = word[i:] if( len(word[:i]) < 2 and not word[:i].isnumeric() ) else word[:i] + \" \" + word[i:]\n",
    "                        break\n",
    "\n",
    "            return(word)\n",
    "    \n",
    "        \n",
    "        def clean_text(self,text: str) -> str:\n",
    "            try:\n",
    "                special_chars = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "                stemmer = PorterStemmer()\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                if not(isinstance(text, str)): text = str(text)\n",
    "\n",
    "                #Removing unprintable characters\n",
    "                text = ''.join(x for x in text if x.isprintable())\n",
    "\n",
    "                # Cleaning the urls\n",
    "                text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "                # Cleaning the html elements\n",
    "                text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "                # Removing the punctuations\n",
    "                text = re.sub('[!#?,.:\";-@#$%^&*_~<>()-]', '', text)\n",
    "\n",
    "\n",
    "                # Removing stop words\n",
    "                text = ' '.join([word for word in text.split() if word not in self.stopword_list])\n",
    "\n",
    "                # Expanding noisy concatenations (Eg: algorithmआणि  -> algorithm आणि ) \n",
    "                text = ' '.join([self.expand_concatenations(word) for word in text.split()])\n",
    "\n",
    "#                 preprocessed_text = \"\"\n",
    "\n",
    "#                 for word in text.split(): \n",
    "#                     if (re.match('\\d+', word)):\n",
    "#                         if(word.isnumeric()):\n",
    "#                             preprocessed_text = preprocessed_text + '#N' + \" \"\n",
    "#                         else:\n",
    "#                             preprocessed_text = preprocessed_text + word.lower() + \" \"\n",
    "\n",
    "#                     else:\n",
    "#                         if(re.match('[a-zA-Z]+', word)):\n",
    "#                             if not len(word) < 2:\n",
    "#                                 word = word.lower()\n",
    "#     #                             word = lemmatizer.lemmatize(word, pos='v')\n",
    "#                                 preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "#                         else:\n",
    "#                             preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "#                 return preprocessed_text\n",
    "                return text\n",
    "            \n",
    "            except ValueError as ve:\n",
    "                print('Error processing:\\t',text)\n",
    "                return ''\n",
    "    \n",
    "        def preprocess_text(self,text: str) -> str:\n",
    "\n",
    "            try:\n",
    "                if not(isinstance(text, str)): text = str(text)\n",
    "                preprocessed_text = \"\"\n",
    "\n",
    "                for word in text.split(): \n",
    "                    if (re.match('\\d+', word)):\n",
    "                        if(word.isnumeric()):\n",
    "                            preprocessed_text = preprocessed_text + '#N' + \" \"\n",
    "                        else:\n",
    "                            preprocessed_text = preprocessed_text + word.lower() + \" \"\n",
    "\n",
    "                    else:\n",
    "                        if(re.match('[a-zA-Z]+', word)):\n",
    "                            if not len(word) < 2:\n",
    "                                word = word.lower()\n",
    "    #                             word = lemmatizer.lemmatize(word, pos='v')\n",
    "                                preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "                        else:\n",
    "                            preprocessed_text = preprocessed_text + word + \" \"\n",
    "\n",
    "                return preprocessed_text\n",
    "\n",
    "            except ValueError as ve:\n",
    "                print('Error processing:\\t',text)\n",
    "                return ''\n",
    "            \n",
    "        def split_devanagri_word(self,word: str, punctuations = True) -> str:\n",
    "            try:\n",
    "                q = Queue()\n",
    "                if not(isinstance(word, str)): word = str(word)\n",
    "                tokens = []\n",
    "                \n",
    "                for char in word:\n",
    "#                     print(char, '--->', unicodedata.name(char))\n",
    "\n",
    "                    if 'letter' in unicodedata.name(char).lower():\n",
    "                        if q.empty():\n",
    "                            tokens.append(char)\n",
    "                        else:\n",
    "                            while not q.empty():\n",
    "                                tokens[len(tokens)-1] += q.get() \n",
    "                            tokens.append(char)   \n",
    "                    else:\n",
    "                        if punctuations == True:\n",
    "                            q.put(char)\n",
    "\n",
    "                while not q.empty():\n",
    "                    tokens[len(tokens)-1] += q.get() \n",
    "                \n",
    "                return tokens\n",
    "                \n",
    "            except ValueError as ve:\n",
    "                print('Error processing:\\t',text)\n",
    "                return ''\n",
    "        \n",
    "        def text2characters(self,text:str, punctuations = True)->str:\n",
    "            try:\n",
    "                if not(isinstance(text, str)): text = str(text)\n",
    "                char_sequence = \"\"\n",
    "                char_list = []\n",
    "                \n",
    "                for word in text.split():\n",
    "                    seq = ' '.join([char for char in self.split_devanagri_word(word, punctuations)])                \n",
    "                    char_sequence = char_sequence + seq + ' '\n",
    "    \n",
    "#                     print(word,'--->',seq)\n",
    "                    \n",
    "                return char_sequence\n",
    "            \n",
    "            except ValueError as ve:\n",
    "                print('Error processing:\\t',text)\n",
    "                return ''\n",
    "            \n",
    "            \n",
    "        def tokenize_characters(self, document):\n",
    "            vocab = set()\n",
    "            cnt = 0\n",
    "            token_dict = {}\n",
    "            \n",
    "            if isinstance(document, list):\n",
    "                for text in document:\n",
    "                    char_sequence = self.text2characters(text)\n",
    "                    tokens_indic = pd.Series(trivial_tokenize_indic(char_sequence))\n",
    "                    word_counts = tokens_indic.value_counts()\n",
    "                    \n",
    "                    vocab = vocab.union(set(word_counts.keys()))\n",
    "\n",
    "                print('Total Unique Tokens (Characters): {}'.format(len(vocab)))\n",
    "\n",
    "                for char in vocab:\n",
    "                    cnt += 1\n",
    "                    token_dict[char] = cnt\n",
    "            \n",
    "            else:\n",
    "                char_sequence = self.text2characters(document)\n",
    "                tokens_indic = pd.Series(trivial_tokenize_indic(char_sequence))\n",
    "                word_counts = tokens_indic.value_counts()  \n",
    "                vocab = vocab.union(set(word_counts.keys()))\n",
    "\n",
    "                print('Total Unique Tokens (Characters): {}'.format(len(vocab)))\n",
    "\n",
    "                for char in vocab:\n",
    "                    cnt += 1\n",
    "                    token_dict[char] = cnt\n",
    "                \n",
    "            return token_dict\n",
    "\n",
    "        \n",
    "        def text_to_sequence(self,document,token_dict):\n",
    "            \n",
    "            sequence_doc = []\n",
    "            \n",
    "            if isinstance(document, list):\n",
    "                print('Total records: ',len(document))\n",
    "                cnt = 0\n",
    "                for text in document:\n",
    "                    char_array = self.text2characters(text).split()\n",
    "                    text_sequence = [token_dict[x] for x in char_array]\n",
    "                    sequence_doc.append(text_sequence)\n",
    "                    cnt+=1\n",
    "                print('Records converted: ',cnt)\n",
    "                \n",
    "            else:\n",
    "                char_array = self.text2characters(document).split()\n",
    "                text_sequence = [token_dict[x] for x in char_array]\n",
    "                sequence_doc.append(text_sequence)\n",
    "                print('Records converted: 1')\n",
    "                \n",
    "            return sequence_doc\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['तर , इंटरप्ट डिस्क्रिप्टर टेबलची सामग्री काय आहे ?']\n",
      "त्यांना CO2 2H20 सीओ२ लागेल99 , Animalत्यांना त्यांनाAnimal Analogy_त्यांना Science२१ १२Number \t--->\t त्यांना CO2  2H20 सीओ२ लागेल 99 Animal त्यांना त्यांना Animal Analogy त्यांना Science २१ १२ Number \n",
      "\n",
      "!@)$&%!#)&$!&$!$I am Atharva ११.२२ Kulkarni 11.22 a B 1 सी \t--->\t I am Atharva ११२२ Kulkarni  1122 a B  1 सी \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('../Technodifacation/Data/training_data_marathi.csv')\n",
    "    \n",
    "    sampletext1 = df['text'].sample().values\n",
    "    print(sampletext1)\n",
    "    pp = Preprocess([])\n",
    "    sampletext2 = 'त्यांना जनतेला पटवून द्यावे लागेल99'\n",
    "\n",
    "    test_list1 = ['त्यांना','H20', '2H20','Animal2Animal' ,'सी२ओ२', 'लागेल99', 'Animalत्यांना',\n",
    "                 'त्यांनाAnimal', 'Analogy_त्यांना', 'Science२१', '१२Number', '!@)$&%!#)&$!&$!$B Bo ', '११.२२','I', '१','1','11.22','a','B','सी']\n",
    "\n",
    "    test_list2 = ['त्यांना CO2 2H20 सीओ२ लागेल99 , Animalत्यांना त्यांनाAnimal Analogy_त्यांना Science२१ १२Number',\n",
    "                 '!@)$&%!#)&$!&$!$I am Atharva ११.२२ Kulkarni 11.22 a B 1 सी']\n",
    "\n",
    "    for text in test_list2:\n",
    "        print(text, '\\t--->\\t', pp.clean_text(text),'\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = Preprocess([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['हि', 'र', 'ड्', 'यां', 'च्', 'या']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_word =  \"हिरड्यांच्या\"\n",
    "tokens = pp.split_devanagri_word(sample_word, punctuations=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "पहिला ---> प हि ला\n",
      "स्तंभ ---> स् तं भ\n",
      "आपल्याला ---> आ प ल् या ला\n",
      "अंदाज ---> अं दा ज\n",
      "देतो ---> दे तो\n",
      "पहिला ---> प ह ल\n",
      "स्तंभ ---> स त भ\n",
      "आपल्याला ---> आ प ल य ल\n",
      "अंदाज ---> अ द ज\n",
      "देतो ---> द त\n",
      "\n",
      "Text:  पहिला स्तंभ आपल्याला अंदाज देतो \n",
      "\n",
      "With Punctuations:  प हि ला स् तं भ आ प ल् या ला अं दा ज दे तो  \n",
      "\n",
      "Only Letters:  प ह ल स त भ आ प ल य ल अ द ज द त \n"
     ]
    }
   ],
   "source": [
    "text = 'पहिला,  स्तंभ आपल्याला अंदाज देतो.'\n",
    "\n",
    "clean_text = pp.clean_text(text)\n",
    "\n",
    "char_sequence_1 = pp.text2characters(clean_text)\n",
    "char_sequence_2 = pp.text2characters(clean_text, punctuations=False)\n",
    "print('\\nText: ',clean_text,'\\n\\nWith Punctuations: ',char_sequence_1,'\\n\\nOnly Letters: ',char_sequence_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Idiotic Keras<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('प', 2), ('ह', 1), ('ि', 1), ('ल', 3), ('ा', 4), ('स', 1), ('्', 2), ('त', 2), ('ं', 2), ('भ', 1), ('आ', 1), ('य', 1), ('अ', 1), ('द', 2), ('ज', 1), ('े', 1), ('ो', 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level = False, split = \" \")\n",
    "\n",
    "tokenizer.fit_on_texts(char_sequence_1)\n",
    "\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Max Jugaad<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "प     2\n",
      "ला    2\n",
      "अं    1\n",
      "भ     1\n",
      "तो    1\n",
      "दे    1\n",
      "दा    1\n",
      "स्    1\n",
      "ज     1\n",
      "तं    1\n",
      "या    1\n",
      "ल्    1\n",
      "हि    1\n",
      "आ     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize.indic_tokenize import trivial_tokenize_indic\n",
    "\n",
    "tokens_indic = trivial_tokenize_indic(char_sequence_1)\n",
    "\n",
    "tokens_indic = pd.Series(tokens_indic)\n",
    "\n",
    "word_counts = tokens_indic.value_counts()\n",
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Converting Devanagri Text to Char array<h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Tokens (Characters): 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ला': 1,\n",
       " 'स्': 2,\n",
       " 'हि': 3,\n",
       " 'ल्': 4,\n",
       " 'या': 5,\n",
       " 'तं': 6,\n",
       " 'ज': 7,\n",
       " 'दा': 8,\n",
       " 'दे': 9,\n",
       " 'आ': 10,\n",
       " 'तो': 11,\n",
       " 'भ': 12,\n",
       " 'अं': 13,\n",
       " 'प': 14}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict = pp.tokenize_characters(clean_text)\n",
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records converted: 1\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[14, 3, 1, 2, 6, 12, 10, 14, 4, 5, 1, 13, 8, 7, 9, 11]]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_seq = pp.text_to_sequence(clean_text, token_dict)\n",
    "print(len(text_seq))\n",
    "text_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
